{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing - Home Credit Default Risk\n",
        "\n",
        "## Goal\n",
        "This notebook shows how to clean and prepare data for machine learning.\n",
        "\n",
        "## Steps:\n",
        "1. Load data\n",
        "2. Handle missing values\n",
        "3. Encode categorical variables\n",
        "4. Scale numerical features\n",
        "5. Save processed data\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📚 Библиотеки успешно импортированы!\n",
            "Pandas version: 2.3.2\n",
            "NumPy version: 2.3.3\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import os\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"Set2\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "print(\"Loading data...\")\n",
        "\n",
        "# Load training data\n",
        "train_df = pd.read_csv('../data_raw/application_train.csv')\n",
        "print(f\"Training data loaded: {train_df.shape}\")\n",
        "\n",
        "# Load test data\n",
        "test_df = pd.read_csv('../data_raw/application_test.csv')\n",
        "print(f\"Test data loaded: {test_df.shape}\")\n",
        "\n",
        "# Load column descriptions\n",
        "columns_desc = pd.read_csv('../data_raw/HomeCredit_columns_description.csv')\n",
        "print(f\"Column descriptions loaded: {columns_desc.shape}\")\n",
        "\n",
        "print(\"\\nData overview:\")\n",
        "print(f\"Total records: {len(train_df):,}\")\n",
        "print(f\"Number of features: {train_df.shape[1]}\")\n",
        "print(f\"Target variable: {train_df['TARGET'].value_counts().to_dict()}\")\n",
        "print(f\"Default rate: {train_df['TARGET'].mean()*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check data types\n",
        "print(\"Data types:\")\n",
        "print(train_df.dtypes.value_counts())\n",
        "\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Handle Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check missing values\n",
        "print(\"Missing values analysis:\")\n",
        "\n",
        "# Count missing values\n",
        "missing_data = train_df.isnull().sum()\n",
        "missing_percent = (missing_data / len(train_df)) * 100\n",
        "\n",
        "# Create DataFrame with missing values info\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_data.index,\n",
        "    'Missing_Count': missing_data.values,\n",
        "    'Missing_Percent': missing_percent.values\n",
        "})\n",
        "\n",
        "# Sort by missing percentage\n",
        "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Percent', ascending=False)\n",
        "\n",
        "print(f\"\\nColumns with missing values ({len(missing_df)} out of {len(train_df.columns)}):\")\n",
        "print(missing_df.head(20))\n",
        "\n",
        "# Plot missing values\n",
        "plt.figure(figsize=(12, 8))\n",
        "if len(missing_df) > 0:\n",
        "    top_missing = missing_df.head(15)\n",
        "    plt.barh(range(len(top_missing)), top_missing['Missing_Percent'])\n",
        "    plt.yticks(range(len(top_missing)), top_missing['Column'])\n",
        "    plt.xlabel('Missing Values (%)')\n",
        "    plt.title('Top 15 Columns with Most Missing Values')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No missing values found!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Process Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create copies for processing\n",
        "train_processed = train_df.copy()\n",
        "test_processed = test_df.copy()\n",
        "\n",
        "print(\"Processing missing values...\")\n",
        "\n",
        "# Function to handle missing values\n",
        "def handle_missing_values(df, is_train=True):\n",
        "    \"\"\"\n",
        "    Handle missing values in dataset\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # 1. Remove columns with >50% missing values\n",
        "    high_missing_cols = []\n",
        "    for col in df_processed.columns:\n",
        "        if df_processed[col].isnull().sum() / len(df_processed) > 0.5:\n",
        "            high_missing_cols.append(col)\n",
        "    \n",
        "    if high_missing_cols:\n",
        "        print(f\"Removing columns with >50% missing values: {high_missing_cols}\")\n",
        "        df_processed = df_processed.drop(columns=high_missing_cols)\n",
        "    \n",
        "    # 2. Fill numerical columns with median\n",
        "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "    numeric_missing = [col for col in numeric_cols if df_processed[col].isnull().any()]\n",
        "    \n",
        "    if numeric_missing:\n",
        "        print(f\"Filling numerical columns with median: {len(numeric_missing)} columns\")\n",
        "        for col in numeric_missing:\n",
        "            df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
        "    \n",
        "    # 3. Fill categorical columns with mode\n",
        "    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
        "    categorical_missing = [col for col in categorical_cols if df_processed[col].isnull().any()]\n",
        "    \n",
        "    if categorical_missing:\n",
        "        print(f\"Filling categorical columns with mode: {len(categorical_missing)} columns\")\n",
        "        for col in categorical_missing:\n",
        "            mode_value = df_processed[col].mode()[0] if not df_processed[col].mode().empty else 'Unknown'\n",
        "            df_processed[col].fillna(mode_value, inplace=True)\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "# Process data\n",
        "train_processed = handle_missing_values(train_processed, is_train=True)\n",
        "test_processed = handle_missing_values(test_processed, is_train=False)\n",
        "\n",
        "print(f\"\\nProcessing completed!\")\n",
        "print(f\"Train size after processing: {train_processed.shape}\")\n",
        "print(f\"Test size after processing: {test_processed.shape}\")\n",
        "\n",
        "# Check remaining missing values\n",
        "remaining_missing_train = train_processed.isnull().sum().sum()\n",
        "remaining_missing_test = test_processed.isnull().sum().sum()\n",
        "\n",
        "print(f\"\\nRemaining missing values:\")\n",
        "print(f\"Train: {remaining_missing_train}\")\n",
        "print(f\"Test: {remaining_missing_test}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Encode Categorical Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check categorical variables\n",
        "print(\"Categorical variables analysis:\")\n",
        "\n",
        "categorical_cols = train_processed.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"Found {len(categorical_cols)} categorical columns:\")\n",
        "for col in categorical_cols:\n",
        "    unique_count = train_processed[col].nunique()\n",
        "    print(f\"  - {col}: {unique_count} unique values\")\n",
        "\n",
        "# Function to encode categorical variables\n",
        "def encode_categorical_variables(df_train, df_test):\n",
        "    \"\"\"\n",
        "    Encode categorical variables using LabelEncoder\n",
        "    \"\"\"\n",
        "    df_train_encoded = df_train.copy()\n",
        "    df_test_encoded = df_test.copy()\n",
        "    \n",
        "    categorical_cols = df_train_encoded.select_dtypes(include=['object']).columns.tolist()\n",
        "    label_encoders = {}\n",
        "    \n",
        "    print(f\"\\nEncoding {len(categorical_cols)} categorical columns...\")\n",
        "    \n",
        "    for col in categorical_cols:\n",
        "        if col in df_train_encoded.columns and col in df_test_encoded.columns:\n",
        "            # Create LabelEncoder\n",
        "            le = LabelEncoder()\n",
        "            \n",
        "            # Combine unique values from train and test for fitting\n",
        "            all_values = pd.concat([df_train_encoded[col], df_test_encoded[col]]).astype(str)\n",
        "            le.fit(all_values)\n",
        "            \n",
        "            # Apply encoding\n",
        "            df_train_encoded[col] = le.transform(df_train_encoded[col].astype(str))\n",
        "            df_test_encoded[col] = le.transform(df_test_encoded[col].astype(str))\n",
        "            \n",
        "            label_encoders[col] = le\n",
        "            print(f\"  Done {col}: {len(le.classes_)} classes\")\n",
        "    \n",
        "    return df_train_encoded, df_test_encoded, label_encoders\n",
        "\n",
        "# Apply encoding\n",
        "train_encoded, test_encoded, encoders = encode_categorical_variables(train_processed, test_processed)\n",
        "\n",
        "print(f\"\\nEncoding completed!\")\n",
        "print(f\"Train size after encoding: {train_encoded.shape}\")\n",
        "print(f\"Test size after encoding: {test_encoded.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Scale Numerical Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for scaling\n",
        "print(\"Preparing data for scaling...\")\n",
        "\n",
        "# Exclude ID and target variable from scaling\n",
        "exclude_cols = ['SK_ID_CURR', 'TARGET']\n",
        "feature_cols = [col for col in train_encoded.columns if col not in exclude_cols]\n",
        "\n",
        "print(f\"Number of features to scale: {len(feature_cols)}\")\n",
        "\n",
        "# Separate numerical and categorical features\n",
        "numeric_features = train_encoded[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"Numerical features: {len(numeric_features)}\")\n",
        "\n",
        "# Function to scale features\n",
        "def scale_features(df_train, df_test, feature_cols):\n",
        "    \"\"\"\n",
        "    Scale numerical features\n",
        "    \"\"\"\n",
        "    df_train_scaled = df_train.copy()\n",
        "    df_test_scaled = df_test.copy()\n",
        "    \n",
        "    # Initialize StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    \n",
        "    # Fit on train data\n",
        "    df_train_scaled[feature_cols] = scaler.fit_transform(df_train[feature_cols])\n",
        "    \n",
        "    # Apply to test data\n",
        "    df_test_scaled[feature_cols] = scaler.transform(df_test[feature_cols])\n",
        "    \n",
        "    return df_train_scaled, df_test_scaled, scaler\n",
        "\n",
        "# Apply scaling\n",
        "train_scaled, test_scaled, scaler = scale_features(train_encoded, test_encoded, numeric_features)\n",
        "\n",
        "print(f\"\\nScaling completed!\")\n",
        "print(f\"Train size after scaling: {train_scaled.shape}\")\n",
        "print(f\"Test size after scaling: {test_scaled.shape}\")\n",
        "\n",
        "# Check statistics after scaling\n",
        "print(f\"\\nStatistics after scaling (first 5 numerical features):\")\n",
        "print(train_scaled[numeric_features[:5]].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Processed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save processed data\n",
        "print(\"Saving processed data...\")\n",
        "\n",
        "# Create directory for processed data\n",
        "os.makedirs('../data_processed', exist_ok=True)\n",
        "\n",
        "# Save train data\n",
        "train_scaled.to_csv('../data_processed/train_processed.csv', index=False)\n",
        "print(\"train_processed.csv saved\")\n",
        "\n",
        "# Save test data\n",
        "test_scaled.to_csv('../data_processed/test_processed.csv', index=False)\n",
        "print(\"test_processed.csv saved\")\n",
        "\n",
        "# Save feature information\n",
        "feature_info = {\n",
        "    'total_features': len(feature_cols),\n",
        "    'numeric_features': len(numeric_features),\n",
        "    'categorical_features': len(categorical_cols),\n",
        "    'feature_names': feature_cols,\n",
        "    'numeric_feature_names': numeric_features,\n",
        "    'categorical_feature_names': categorical_cols\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('../data_processed/feature_info.json', 'w') as f:\n",
        "    json.dump(feature_info, f, indent=2)\n",
        "print(\"feature_info.json saved\")\n",
        "\n",
        "print(f\"\\nFinal statistics:\")\n",
        "print(f\"Original train size: {train_df.shape}\")\n",
        "print(f\"Processed train size: {train_scaled.shape}\")\n",
        "print(f\"Original test size: {test_df.shape}\")\n",
        "print(f\"Processed test size: {test_scaled.shape}\")\n",
        "print(f\"Number of features: {len(feature_cols)}\")\n",
        "print(f\"Target variable (default): {train_scaled['TARGET'].mean()*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nData preprocessing completed successfully!\")\n",
        "print(f\"Processed data saved in: ../data_processed/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (data-mining)",
      "language": "python",
      "name": "data-mining"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
