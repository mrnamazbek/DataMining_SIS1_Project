{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìä Data Preprocessing - Home Credit Default Risk\n",
        "\n",
        "## üéØ –¶–µ–ª—å –Ω–æ—É—Ç–±—É–∫–∞\n",
        "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–æ–ª–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ –¥–µ—Ñ–æ–ª—Ç–∞.\n",
        "\n",
        "## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:\n",
        "1. **–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö**\n",
        "2. **–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π**\n",
        "3. **–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö**\n",
        "4. **–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤**\n",
        "5. **–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import os\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"üìö –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–µ—Ä–≤–∏—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "print(\"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ...\")\n",
        "\n",
        "# –û—Å–Ω–æ–≤–Ω–æ–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "train_df = pd.read_csv('../data_raw/application_train.csv')\n",
        "print(f\"‚úÖ application_train.csv –∑–∞–≥—Ä—É–∂–µ–Ω: {train_df.shape}\")\n",
        "\n",
        "# –¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
        "test_df = pd.read_csv('../data_raw/application_test.csv')\n",
        "print(f\"‚úÖ application_test.csv –∑–∞–≥—Ä—É–∂–µ–Ω: {test_df.shape}\")\n",
        "\n",
        "# –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫\n",
        "columns_desc = pd.read_csv('../data_raw/HomeCredit_columns_description.csv')\n",
        "print(f\"‚úÖ HomeCredit_columns_description.csv –∑–∞–≥—Ä—É–∂–µ–Ω: {columns_desc.shape}\")\n",
        "\n",
        "print(\"\\nüìä –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "print(f\"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(train_df):,}\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {train_df.shape[1]}\")\n",
        "print(f\"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (TARGET): {train_df['TARGET'].value_counts().to_dict()}\")\n",
        "print(f\"–ü—Ä–æ—Ü–µ–Ω—Ç –¥–µ—Ñ–æ–ª—Ç–æ–≤: {train_df['TARGET'].mean()*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö\n",
        "print(\"üîç –ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "print(\"\\n–¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "print(train_df.dtypes.value_counts())\n",
        "\n",
        "print(\"\\nüìã –ü–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "print(\"üîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:\")\n",
        "\n",
        "# –ü–æ–¥—Å—á–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "missing_data = train_df.isnull().sum()\n",
        "missing_percent = (missing_data / len(train_df)) * 100\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º DataFrame —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_data.index,\n",
        "    'Missing_Count': missing_data.values,\n",
        "    'Missing_Percent': missing_percent.values\n",
        "})\n",
        "\n",
        "# –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–æ—Ü–µ–Ω—Ç—É –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Percent', ascending=False)\n",
        "\n",
        "print(f\"\\nüìä –ö–æ–ª–æ–Ω–∫–∏ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ ({len(missing_df)} –∏–∑ {len(train_df.columns)}):\")\n",
        "print(missing_df.head(20))\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "plt.figure(figsize=(12, 8))\n",
        "if len(missing_df) > 0:\n",
        "    top_missing = missing_df.head(15)\n",
        "    plt.barh(range(len(top_missing)), top_missing['Missing_Percent'])\n",
        "    plt.yticks(range(len(top_missing)), top_missing['Column'])\n",
        "    plt.xlabel('–ü—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (%)')\n",
        "    plt.title('–¢–æ–ø-15 –∫–æ–ª–æ–Ω–æ–∫ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏\n",
        "train_processed = train_df.copy()\n",
        "test_processed = test_df.copy()\n",
        "\n",
        "print(\"üîß –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π...\")\n",
        "\n",
        "# –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "def handle_missing_values(df, is_train=True):\n",
        "    \"\"\"\n",
        "    –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # 1. –ö–æ–ª–æ–Ω–∫–∏ —Å –±–æ–ª–µ–µ —á–µ–º 50% –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π - —É–¥–∞–ª—è–µ–º\n",
        "    high_missing_cols = []\n",
        "    for col in df_processed.columns:\n",
        "        if df_processed[col].isnull().sum() / len(df_processed) > 0.5:\n",
        "            high_missing_cols.append(col)\n",
        "    \n",
        "    if high_missing_cols:\n",
        "        print(f\"üóëÔ∏è –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å >50% –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {high_missing_cols}\")\n",
        "        df_processed = df_processed.drop(columns=high_missing_cols)\n",
        "    \n",
        "    # 2. –ß–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ - –∑–∞–ø–æ–ª–Ω—è–µ–º –º–µ–¥–∏–∞–Ω–æ–π\n",
        "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "    numeric_missing = [col for col in numeric_cols if df_processed[col].isnull().any()]\n",
        "    \n",
        "    if numeric_missing:\n",
        "        print(f\"üî¢ –ó–∞–ø–æ–ª–Ω—è–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –º–µ–¥–∏–∞–Ω–æ–π: {len(numeric_missing)} –∫–æ–ª–æ–Ω–æ–∫\")\n",
        "        for col in numeric_missing:\n",
        "            df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
        "    \n",
        "    # 3. –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ - –∑–∞–ø–æ–ª–Ω—è–µ–º –º–æ–¥–æ–π\n",
        "    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
        "    categorical_missing = [col for col in categorical_cols if df_processed[col].isnull().any()]\n",
        "    \n",
        "    if categorical_missing:\n",
        "        print(f\"üìù –ó–∞–ø–æ–ª–Ω—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –º–æ–¥–æ–π: {len(categorical_missing)} –∫–æ–ª–æ–Ω–æ–∫\")\n",
        "        for col in categorical_missing:\n",
        "            mode_value = df_processed[col].mode()[0] if not df_processed[col].mode().empty else 'Unknown'\n",
        "            df_processed[col].fillna(mode_value, inplace=True)\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "train_processed = handle_missing_values(train_processed, is_train=True)\n",
        "test_processed = handle_missing_values(test_processed, is_train=False)\n",
        "\n",
        "print(f\"\\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")\n",
        "print(f\"–†–∞–∑–º–µ—Ä train –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {train_processed.shape}\")\n",
        "print(f\"–†–∞–∑–º–µ—Ä test –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {test_processed.shape}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ—Å—Ç–∞–ª–∏—Å—å –ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "remaining_missing_train = train_processed.isnull().sum().sum()\n",
        "remaining_missing_test = test_processed.isnull().sum().sum()\n",
        "\n",
        "print(f\"\\nüîç –û—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:\")\n",
        "print(f\"Train: {remaining_missing_train}\")\n",
        "print(f\"Test: {remaining_missing_test}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n",
        "print(\"üîç –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö:\")\n",
        "\n",
        "categorical_cols = train_processed.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"–ù–∞–π–¥–µ–Ω–æ {len(categorical_cols)} –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫:\")\n",
        "for col in categorical_cols:\n",
        "    unique_count = train_processed[col].nunique()\n",
        "    print(f\"  - {col}: {unique_count} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\")\n",
        "\n",
        "# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n",
        "def encode_categorical_variables(df_train, df_test):\n",
        "    \"\"\"\n",
        "    –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LabelEncoder\n",
        "    \"\"\"\n",
        "    df_train_encoded = df_train.copy()\n",
        "    df_test_encoded = df_test.copy()\n",
        "    \n",
        "    categorical_cols = df_train_encoded.select_dtypes(include=['object']).columns.tolist()\n",
        "    label_encoders = {}\n",
        "    \n",
        "    print(f\"\\nüîß –ö–æ–¥–∏—Ä—É–µ–º {len(categorical_cols)} –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫...\")\n",
        "    \n",
        "    for col in categorical_cols:\n",
        "        if col in df_train_encoded.columns and col in df_test_encoded.columns:\n",
        "            # –°–æ–∑–¥–∞–µ–º LabelEncoder\n",
        "            le = LabelEncoder()\n",
        "            \n",
        "            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ train –∏ test –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "            all_values = pd.concat([df_train_encoded[col], df_test_encoded[col]]).astype(str)\n",
        "            le.fit(all_values)\n",
        "            \n",
        "            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "            df_train_encoded[col] = le.transform(df_train_encoded[col].astype(str))\n",
        "            df_test_encoded[col] = le.transform(df_test_encoded[col].astype(str))\n",
        "            \n",
        "            label_encoders[col] = le\n",
        "            print(f\"  ‚úÖ {col}: {len(le.classes_)} –∫–ª–∞—Å—Å–æ–≤\")\n",
        "    \n",
        "    return df_train_encoded, df_test_encoded, label_encoders\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "train_encoded, test_encoded, encoders = encode_categorical_variables(train_processed, test_processed)\n",
        "\n",
        "print(f\"\\n‚úÖ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
        "print(f\"–†–∞–∑–º–µ—Ä train –ø–æ—Å–ª–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {train_encoded.shape}\")\n",
        "print(f\"–†–∞–∑–º–µ—Ä test –ø–æ—Å–ª–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {test_encoded.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "print(\"üîß –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è...\")\n",
        "\n",
        "# –ò—Å–∫–ª—é—á–∞–µ–º ID –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –∏–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "exclude_cols = ['SK_ID_CURR', 'TARGET']\n",
        "feature_cols = [col for col in train_encoded.columns if col not in exclude_cols]\n",
        "\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: {len(feature_cols)}\")\n",
        "\n",
        "# –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "numeric_features = train_encoded[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"–ß–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(numeric_features)}\")\n",
        "\n",
        "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é StandardScaler\n",
        "def scale_features(df_train, df_test, feature_cols):\n",
        "    \"\"\"\n",
        "    –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    \"\"\"\n",
        "    df_train_scaled = df_train.copy()\n",
        "    df_test_scaled = df_test.copy()\n",
        "    \n",
        "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    \n",
        "    # –û–±—É—á–∞–µ–º –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö\n",
        "    df_train_scaled[feature_cols] = scaler.fit_transform(df_train[feature_cols])\n",
        "    \n",
        "    # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ test –¥–∞–Ω–Ω—ã–º\n",
        "    df_test_scaled[feature_cols] = scaler.transform(df_test[feature_cols])\n",
        "    \n",
        "    return df_train_scaled, df_test_scaled, scaler\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "train_scaled, test_scaled, scaler = scale_features(train_encoded, test_encoded, numeric_features)\n",
        "\n",
        "print(f\"\\n‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
        "print(f\"–†–∞–∑–º–µ—Ä train –ø–æ—Å–ª–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: {train_scaled.shape}\")\n",
        "print(f\"–†–∞–∑–º–µ—Ä test –ø–æ—Å–ª–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è: {test_scaled.shape}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ—Å–ª–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ—Å–ª–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–µ—Ä–≤—ã–µ 5 —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤):\")\n",
        "print(train_scaled[numeric_features[:5]].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6Ô∏è‚É£ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "print(\"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ...\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "os.makedirs('../data_processed', exist_ok=True)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º train –¥–∞–Ω–Ω—ã–µ\n",
        "train_scaled.to_csv('../data_processed/train_processed.csv', index=False)\n",
        "print(\"‚úÖ train_processed.csv —Å–æ—Ö—Ä–∞–Ω–µ–Ω\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º test –¥–∞–Ω–Ω—ã–µ\n",
        "test_scaled.to_csv('../data_processed/test_processed.csv', index=False)\n",
        "print(\"‚úÖ test_processed.csv —Å–æ—Ö—Ä–∞–Ω–µ–Ω\")\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
        "feature_info = {\n",
        "    'total_features': len(feature_cols),\n",
        "    'numeric_features': len(numeric_features),\n",
        "    'categorical_features': len(categorical_cols),\n",
        "    'feature_names': feature_cols,\n",
        "    'numeric_feature_names': numeric_features,\n",
        "    'categorical_feature_names': categorical_cols\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('../data_processed/feature_info.json', 'w') as f:\n",
        "    json.dump(feature_info, f, indent=2)\n",
        "print(\"‚úÖ feature_info.json —Å–æ—Ö—Ä–∞–Ω–µ–Ω\")\n",
        "\n",
        "print(f\"\\nüìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
        "print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä train: {train_df.shape}\")\n",
        "print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä train: {train_scaled.shape}\")\n",
        "print(f\"–ò—Å—Ö–æ–¥–Ω—ã–π —Ä–∞–∑–º–µ—Ä test: {test_df.shape}\")\n",
        "print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä test: {test_scaled.shape}\")\n",
        "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_cols)}\")\n",
        "print(f\"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (–¥–µ—Ñ–æ–ª—Ç): {train_scaled['TARGET'].mean()*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nüéâ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!\")\n",
        "print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ: ../data_processed/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
