{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Mining Project - Part 2 (SIS 2): Modeling & Evaluation\n",
                "\n",
                "## Goal\n",
                "This notebook covers the second part of the project (SIS 2), focusing on applying data mining techniques, evaluating models, and interpreting results.\n",
                "\n",
                "## Requirements:\n",
                "1. **Data Mining Techniques**: Apply 4-5 algorithms (Classification).\n",
                "2. **Model Evaluation**: Metrics (Accuracy, Precision, Recall, F1, ROC-AUC), Cross-validation.\n",
                "3. **Hyperparameter Tuning**: Optimize model performance.\n",
                "4. **Comparison**: Compare models and select the best one.\n",
                "5. **Interpretation**: Feature importance, Confusion Matrix, ROC Curve.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import json\n",
                "import time\n",
                "\n",
                "# Models\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "\n",
                "# Model Selection & Evaluation\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, StratifiedKFold\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve\n",
                "\n",
                "# Settings\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "plt.style.use('ggplot')\n",
                "pd.set_option('display.max_columns', None)\n",
                "\n",
                "print(\"Libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Data\n",
                "We use the processed data with selected features from Part 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load processed data\n",
                "print(\"Loading processed data...\")\n",
                "df = pd.read_csv('../data_processed/train_processed_selected.csv')\n",
                "print(f\"Data shape: {df.shape}\")\n",
                "\n",
                "# Separate features and target\n",
                "X = df.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
                "y = df['TARGET']\n",
                "\n",
                "print(f\"Features shape: {X.shape}\")\n",
                "print(f\"Target shape: {y.shape}\")\n",
                "\n",
                "# Check class balance\n",
                "print(\"\\nTarget distribution:\")\n",
                "print(y.value_counts(normalize=True))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Train-Test Split\n",
                "We split the data into training (80%) and testing (20%) sets. Stratified split is used due to class imbalance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "print(f\"Training set: {X_train.shape}\")\n",
                "print(f\"Testing set: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Methodology: Algorithms Selection\n",
                "We will apply the following 5 algorithms:\n",
                "1. **Logistic Regression**: A robust baseline for binary classification.\n",
                "2. **Decision Tree**: Provides interpretability and captures non-linear relationships.\n",
                "3. **Random Forest**: An ensemble method that reduces overfitting and improves accuracy.\n",
                "4. **Gradient Boosting (GBM)**: Builds models sequentially to correct errors of previous models.\n",
                "5. **Gaussian Naive Bayes**: A probabilistic classifier, good for high-dimensional data (fast baseline)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize models\n",
                "models = {\n",
                "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced'),\n",
                "    'Decision Tree': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
                "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100, class_weight='balanced', n_jobs=-1),\n",
                "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
                "    'Naive Bayes': GaussianNB()\n",
                "}\n",
                "\n",
                "print(\"Models initialized.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Training & Evaluation (Baseline)\n",
                "We will train each model and evaluate using multiple metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "\n",
                "print(\"Training models...\")\n",
                "for name, model in models.items():\n",
                "    start_time = time.time()\n",
                "    \n",
                "    # Train\n",
                "    model.fit(X_train, y_train)\n",
                "    \n",
                "    # Predict\n",
                "    y_pred = model.predict(X_test)\n",
                "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else np.zeros(len(y_test))\n",
                "    \n",
                "    # Metrics\n",
                "    acc = accuracy_score(y_test, y_pred)\n",
                "    prec = precision_score(y_test, y_pred)\n",
                "    rec = recall_score(y_test, y_pred)\n",
                "    f1 = f1_score(y_test, y_pred)\n",
                "    roc = roc_auc_score(y_test, y_prob)\n",
                "    \n",
                "    elapsed_time = time.time() - start_time\n",
                "    \n",
                "    results.append({\n",
                "        'Model': name,\n",
                "        'Accuracy': acc,\n",
                "        'Precision': prec,\n",
                "        'Recall': rec,\n",
                "        'F1 Score': f1,\n",
                "        'ROC AUC': roc,\n",
                "        'Time (s)': elapsed_time\n",
                "    })\n",
                "    \n",
                "    print(f\"{name} done in {elapsed_time:.2f}s\")\n",
                "\n",
                "# Create DataFrame\n",
                "results_df = pd.DataFrame(results)\n",
                "results_df.sort_values(by='ROC AUC', ascending=False, inplace=True)\n",
                "results_df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Hyperparameter Tuning\n",
                "We will tune the hyperparameters of the **Random Forest** model (or the best performing one) to improve performance. We use `RandomizedSearchCV` for efficiency."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define hyperparameter grid for Random Forest\n",
                "param_dist = {\n",
                "    'n_estimators': [100, 200, 300],\n",
                "    'max_depth': [10, 20, 30, None],\n",
                "    'min_samples_split': [2, 5, 10],\n",
                "    'min_samples_leaf': [1, 2, 4],\n",
                "    'max_features': ['sqrt', 'log2']\n",
                "}\n",
                "\n",
                "print(\"Starting Hyperparameter Tuning for Random Forest...\")\n",
                "rf = RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1)\n",
                "\n",
                "random_search = RandomizedSearchCV(\n",
                "    estimator=rf,\n",
                "    param_distributions=param_dist,\n",
                "    n_iter=10,  # Number of parameter settings that are sampled\n",
                "    cv=3,       # 3-fold cross-validation\n",
                "    scoring='roc_auc',\n",
                "    n_jobs=-1,\n",
                "    random_state=42,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "random_search.fit(X_train, y_train)\n",
                "\n",
                "print(f\"Best Parameters: {random_search.best_params_}\")\n",
                "print(f\"Best ROC AUC Score: {random_search.best_score_:.4f}\")\n",
                "\n",
                "# Get best model\n",
                "best_rf = random_search.best_estimator_"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Impact of Hyperparameters\n",
                "Let's visualize how changing `max_depth` affects the model performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "depths = [5, 10, 15, 20, 25, 30]\n",
                "train_scores = []\n",
                "test_scores = []\n",
                "\n",
                "for d in depths:\n",
                "    clf = RandomForestClassifier(max_depth=d, n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1)\n",
                "    clf.fit(X_train, y_train)\n",
                "    \n",
                "    train_scores.append(roc_auc_score(y_train, clf.predict_proba(X_train)[:, 1]))\n",
                "    test_scores.append(roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1]))\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(depths, train_scores, label='Train AUC', marker='o')\n",
                "plt.plot(depths, test_scores, label='Test AUC', marker='o')\n",
                "plt.xlabel('Max Depth')\n",
                "plt.ylabel('ROC AUC Score')\n",
                "plt.title('Impact of Max Depth on Random Forest Performance')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Final Model Evaluation & Comparison\n",
                "Comparison between Baseline Random Forest and Tuned Random Forest."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate Best Model on Test Set\n",
                "y_pred_best = best_rf.predict(X_test)\n",
                "y_prob_best = best_rf.predict_proba(X_test)[:, 1]\n",
                "\n",
                "print(\"Tuned Random Forest Performance:\")\n",
                "print(classification_report(y_test, y_pred_best))\n",
                "print(f\"ROC AUC: {roc_auc_score(y_test, y_prob_best):.4f}\")\n",
                "\n",
                "# Compare with Baseline\n",
                "baseline_rf = models['Random Forest']\n",
                "y_prob_baseline = baseline_rf.predict_proba(X_test)[:, 1]\n",
                "\n",
                "print(f\"Baseline RF ROC AUC: {roc_auc_score(y_test, y_prob_baseline):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Results & Interpretation\n",
                "### Confusion Matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "cm = confusion_matrix(y_test, y_pred_best)\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Actual')\n",
                "plt.title('Confusion Matrix (Tuned Random Forest)')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ROC Curve Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "\n",
                "# Plot ROC for all models\n",
                "for name, model in models.items():\n",
                "    if hasattr(model, \"predict_proba\"):\n",
                "        y_prob = model.predict_proba(X_test)[:, 1]\n",
                "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
                "        auc = roc_auc_score(y_test, y_prob)\n",
                "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.3f})\")\n",
                "\n",
                "# Plot Tuned RF\n",
                "fpr_best, tpr_best, _ = roc_curve(y_test, y_prob_best)\n",
                "auc_best = roc_auc_score(y_test, y_prob_best)\n",
                "plt.plot(fpr_best, tpr_best, label=f\"Tuned Random Forest (AUC = {auc_best:.3f})\", linestyle='--', linewidth=2, color='black')\n",
                "\n",
                "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
                "plt.xlabel('False Positive Rate')\n",
                "plt.ylabel('True Positive Rate')\n",
                "plt.title('ROC Curve Comparison')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Feature Importance\n",
                "Which features contributed most to the prediction?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "importances = best_rf.feature_importances_\n",
                "indices = np.argsort(importances)[::-1]\n",
                "features = X.columns\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "plt.title(\"Feature Importances (Tuned Random Forest)\")\n",
                "plt.bar(range(X.shape[1]), importances[indices], align=\"center\")\n",
                "plt.xticks(range(X.shape[1]), features[indices], rotation=90)\n",
                "plt.xlim([-1, X.shape[1]])\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Conclusion\n",
                "Summarize the findings, best model, and key drivers of default risk."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}