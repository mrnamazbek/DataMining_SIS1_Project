{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis - Home Credit Default Risk (Selected Features)\n",
        "\n",
        "## Goal\n",
        "This notebook shows how to explore data using only the most important features for predicting loan defaults.\n",
        "\n",
        "## Steps:\n",
        "1. Load processed data with selected features\n",
        "2. Analyze target variable\n",
        "3. Statistical analysis of features\n",
        "4. Correlation analysis\n",
        "5. Visualize distributions\n",
        "6. Find important features\n",
        "7. Draw conclusions\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from scipy import stats\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "import json\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"Set2\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Set plot size\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Plot settings applied!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Processed Data with Selected Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed data\n",
        "print(\"Loading processed data with selected features...\")\n",
        "\n",
        "# Load train data\n",
        "df = pd.read_csv('../data_processed/train_processed_selected.csv')\n",
        "print(f\"Processed data loaded: {df.shape}\")\n",
        "\n",
        "# Load feature information\n",
        "with open('../data_processed/feature_info_selected.json', 'r') as f:\n",
        "    feature_info = json.load(f)\n",
        "\n",
        "print(f\"Data information:\")\n",
        "print(f\"  - Total records: {len(df):,}\")\n",
        "print(f\"  - Number of features: {df.shape[1]}\")\n",
        "print(f\"  - Numerical features: {feature_info['numeric_features']}\")\n",
        "print(f\"  - Categorical features: {feature_info['categorical_features']}\")\n",
        "\n",
        "# Basic data info\n",
        "print(f\"\\nBasic information:\")\n",
        "print(f\"  - Missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"  - Duplicates: {df.duplicated().sum()}\")\n",
        "\n",
        "# Analyze target variable\n",
        "target_stats = df['TARGET'].value_counts()\n",
        "print(f\"\\nTarget variable (TARGET):\")\n",
        "print(f\"  - No default (0): {target_stats[0]:,} ({target_stats[0]/len(df)*100:.2f}%)\")\n",
        "print(f\"  - Default (1): {target_stats[1]:,} ({target_stats[1]/len(df)*100:.2f}%)\")\n",
        "print(f\"  - Imbalance ratio: {target_stats[0]/target_stats[1]:.1f}:1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Target Variable Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize target variable distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Create pie chart\n",
        "plt.subplot(1, 2, 1)\n",
        "target_counts = df['TARGET'].value_counts()\n",
        "plt.pie(target_counts.values, labels=['No Default', 'Default'], autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Target Variable Distribution')\n",
        "\n",
        "# Create bar chart\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(['No Default', 'Default'], target_counts.values, color=['lightblue', 'lightcoral'])\n",
        "plt.title('Target Variable Counts')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Default rate: {df['TARGET'].mean()*100:.2f}%\")\n",
        "print(f\"Class imbalance: {target_counts[0]/target_counts[1]:.1f}:1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Statistical Analysis of Selected Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical analysis of numerical features\n",
        "print(\"Statistical analysis of selected features:\")\n",
        "\n",
        "# Get numerical features (exclude ID and target)\n",
        "numeric_features = [col for col in df.columns if col not in ['SK_ID_CURR', 'TARGET'] and df[col].dtype in ['int64', 'float64']]\n",
        "\n",
        "print(f\"Number of numerical features: {len(numeric_features)}\")\n",
        "\n",
        "# Basic statistics\n",
        "print(\"\\nBasic statistics for numerical features:\")\n",
        "print(df[numeric_features].describe())\n",
        "\n",
        "# Check for highly correlated features\n",
        "print(\"\\nChecking for highly correlated features...\")\n",
        "correlation_matrix = df[numeric_features].corr()\n",
        "\n",
        "# Find highly correlated pairs\n",
        "high_corr_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_value = correlation_matrix.iloc[i, j]\n",
        "        if abs(corr_value) > 0.8:  # High correlation threshold\n",
        "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], corr_value))\n",
        "\n",
        "print(f\"Found {len(high_corr_pairs)} highly correlated pairs (>0.8):\")\n",
        "for pair in high_corr_pairs[:10]:  # Show first 10\n",
        "    print(f\"  {pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Correlation Analysis with Target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation analysis with target variable\n",
        "print(\"Correlation analysis with target variable:\")\n",
        "\n",
        "# Calculate correlations with target\n",
        "target_correlations = df[numeric_features].corrwith(df['TARGET']).abs().sort_values(ascending=False)\n",
        "\n",
        "print(f\"\\nTop 10 features most correlated with TARGET:\")\n",
        "print(target_correlations.head(10))\n",
        "\n",
        "# Visualize correlation heatmap\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "# Select top 15 features for heatmap\n",
        "top_features = target_correlations.head(15).index.tolist()\n",
        "correlation_subset = df[top_features + ['TARGET']].corr()\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(correlation_subset, annot=True, cmap='coolwarm', center=0, \n",
        "            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
        "plt.title('Correlation Heatmap - Top 15 Features with Target')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze correlation with target\n",
        "print(f\"\\nCorrelation analysis summary:\")\n",
        "print(f\"Strong positive correlation (>0.1): {len(target_correlations[target_correlations > 0.1])}\")\n",
        "print(f\"Weak correlation (0.05-0.1): {len(target_correlations[(target_correlations > 0.05) & (target_correlations <= 0.1)])}\")\n",
        "print(f\"Very weak correlation (<0.05): {len(target_correlations[target_correlations <= 0.05])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze distributions of top features\n",
        "print(\"Feature distribution analysis:\")\n",
        "\n",
        "# Select top 6 features for detailed analysis\n",
        "top_6_features = target_correlations.head(6).index.tolist()\n",
        "\n",
        "# Create distribution plots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, feature in enumerate(top_6_features):\n",
        "    # Create histogram for each class\n",
        "    default_data = df[df['TARGET'] == 1][feature]\n",
        "    no_default_data = df[df['TARGET'] == 0][feature]\n",
        "    \n",
        "    axes[i].hist(no_default_data, bins=50, alpha=0.7, label='No Default', color='lightblue', density=True)\n",
        "    axes[i].hist(default_data, bins=50, alpha=0.7, label='Default', color='lightcoral', density=True)\n",
        "    axes[i].set_title(f'{feature}\\n(Corr: {target_correlations[feature]:.3f})')\n",
        "    axes[i].set_xlabel('Value')\n",
        "    axes[i].set_ylabel('Density')\n",
        "    axes[i].legend()\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Box plots for top features\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, feature in enumerate(top_6_features):\n",
        "    sns.boxplot(data=df, x='TARGET', y=feature, ax=axes[i])\n",
        "    axes[i].set_title(f'{feature}\\n(Corr: {target_correlations[feature]:.3f})')\n",
        "    axes[i].set_xlabel('Target (0=No Default, 1=Default)')\n",
        "    axes[i].set_ylabel('Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Important Features Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance analysis using mutual information\n",
        "print(\"Feature importance analysis:\")\n",
        "\n",
        "# Prepare data for mutual information\n",
        "X = df[numeric_features]\n",
        "y = df['TARGET']\n",
        "\n",
        "# Calculate mutual information\n",
        "mi_scores = mutual_info_classif(X, y, random_state=42)\n",
        "mi_df = pd.DataFrame({\n",
        "    'feature': numeric_features,\n",
        "    'mi_score': mi_scores\n",
        "}).sort_values('mi_score', ascending=False)\n",
        "\n",
        "print(f\"\\nTop 10 features by Mutual Information:\")\n",
        "print(mi_df.head(10))\n",
        "\n",
        "# Visualize feature importance\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "\n",
        "# Correlation with target\n",
        "top_corr_features = target_correlations.head(10)\n",
        "ax1.barh(range(len(top_corr_features)), top_corr_features.values)\n",
        "ax1.set_yticks(range(len(top_corr_features)))\n",
        "ax1.set_yticklabels(top_corr_features.index)\n",
        "ax1.set_xlabel('Correlation with Target')\n",
        "ax1.set_title('Top 10 Features by Correlation with Target')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Mutual information\n",
        "top_mi_features = mi_df.head(10)\n",
        "ax2.barh(range(len(top_mi_features)), top_mi_features['mi_score'])\n",
        "ax2.set_yticks(range(len(top_mi_features)))\n",
        "ax2.set_yticklabels(top_mi_features['feature'])\n",
        "ax2.set_xlabel('Mutual Information Score')\n",
        "ax2.set_title('Top 10 Features by Mutual Information')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare correlation and mutual information\n",
        "print(f\"\\nComparison of top features:\")\n",
        "comparison_df = pd.DataFrame({\n",
        "    'feature': top_corr_features.index,\n",
        "    'correlation': top_corr_features.values,\n",
        "    'mi_score': [mi_df[mi_df['feature'] == f]['mi_score'].iloc[0] if f in mi_df['feature'].values else 0 for f in top_corr_features.index]\n",
        "})\n",
        "print(comparison_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Conclusions and Insights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary of findings\n",
        "print(\"=== EDA CONCLUSIONS AND INSIGHTS ===\\n\")\n",
        "\n",
        "print(\"1. DATASET OVERVIEW:\")\n",
        "print(f\"   - Total records: {len(df):,}\")\n",
        "print(f\"   - Number of features: {df.shape[1]}\")\n",
        "print(f\"   - Default rate: {df['TARGET'].mean()*100:.2f}%\")\n",
        "print(f\"   - Class imbalance: {target_stats[0]/target_stats[1]:.1f}:1\")\n",
        "\n",
        "print(f\"\\n2. DATA QUALITY:\")\n",
        "print(f\"   - Missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"   - Duplicates: {df.duplicated().sum()}\")\n",
        "print(f\"   - All data is clean and ready for modeling\")\n",
        "\n",
        "print(f\"\\n3. KEY FINDINGS:\")\n",
        "print(f\"   - Strongest correlation with default: {target_correlations.index[0]} ({target_correlations.iloc[0]:.3f})\")\n",
        "print(f\"   - Top 5 most important features:\")\n",
        "for i in range(5):\n",
        "    print(f\"     {i+1}. {target_correlations.index[i]} (corr: {target_correlations.iloc[i]:.3f})\")\n",
        "\n",
        "print(f\"\\n4. FEATURE CATEGORIES:\")\n",
        "print(f\"   - External data sources (EXT_SOURCE_*) are highly predictive\")\n",
        "print(f\"   - Financial ratios and amounts show strong patterns\")\n",
        "print(f\"   - Customer demographics play important role\")\n",
        "print(f\"   - Credit bureau information is crucial\")\n",
        "\n",
        "print(f\"\\n5. MODELING RECOMMENDATIONS:\")\n",
        "print(f\"   - Use stratified sampling due to class imbalance\")\n",
        "print(f\"   - Focus on top 15-20 features for initial models\")\n",
        "print(f\"   - Consider feature engineering for external sources\")\n",
        "print(f\"   - Apply appropriate evaluation metrics (AUC, Precision-Recall)\")\n",
        "\n",
        "print(f\"\\n6. NEXT STEPS FOR SIS2:\")\n",
        "print(f\"   - Build multiple classification models\")\n",
        "print(f\"   - Compare performance across algorithms\")\n",
        "print(f\"   - Implement feature selection techniques\")\n",
        "print(f\"   - Create ensemble methods for better predictions\")\n",
        "\n",
        "print(f\"\\n=== EDA ANALYSIS COMPLETED ===\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
